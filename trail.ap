from langchain.chains import RetrievalQA
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.manager import CallbackManager
from langchain.llms import Ollama
from langchain.embeddings.ollama import OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
import streamlit as st
import os
import time
import shutil

if not os.path.exists('files'):
    os.mkdir('files')

if not os.path.exists('data'):
    os.mkdir('data')

def clear_existing_data():
    if os.path.exists('data'):
        shutil.rmtree('data')
    os.mkdir('data')
    st.session_state.chat_history = []

def get_or_create_vectorstore():
    if 'vectorstore' not in st.session_state or st.session_state.vectorstore is None:
        embedding_function = OllamaEmbeddings(model="mistral")
        if os.path.exists('data') and os.listdir('data'):
            st.session_state.vectorstore = Chroma(persist_directory='data', embedding_function=embedding_function)
        else:
            st.session_state.vectorstore = Chroma(persist_directory='data', embedding_function=embedding_function)
            st.session_state.vectorstore.persist()
    return st.session_state.vectorstore

# Initialize session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

if 'template' not in st.session_state:
    st.session_state.template = """You are a knowledgeable chatbot, here to help with questions about the information in the uploaded PDF. Your tone should be professional and informative. Always refer to the context provided to answer questions. If the information is not in the context, say so explicitly.

    Context: {context}
    History: {history}

    User: {question}
    Chatbot: Based on the information provided in the PDF context, """

if 'prompt' not in st.session_state:
    st.session_state.prompt = PromptTemplate(
        input_variables=["history", "context", "question"],
        template=st.session_state.template,
    )

if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="history",
        return_messages=True,
        input_key="question")

if 'llm' not in st.session_state:
    st.session_state.llm = Ollama(base_url="http://localhost:11434",
                model="mistral",
                verbose=True,
                callback_manager=CallbackManager(
                    [StreamingStdOutCallbackHandler()]),
                )

st.title("PDF Chatbot")

# Upload a PDF file
uploaded_file = st.file_uploader("Upload your PDF", type='pdf')

for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["message"])

if uploaded_file is not None:
    # Check if this is a new file
    if 'last_uploaded_file' not in st.session_state or st.session_state.last_uploaded_file != uploaded_file.name:
        st.write("New file detected. Clearing existing data.")
        clear_existing_data()
        st.session_state.last_uploaded_file = uploaded_file.name
    else:
        st.write("Using existing data for this file.")

    if not os.path.isfile("files/"+uploaded_file.name+".pdf"):
        with st.status("Analyzing your document..."):
            bytes_data = uploaded_file.read()
            f = open("files/"+uploaded_file.name+".pdf", "wb")
            f.write(bytes_data)
            f.close()
            loader = PyPDFLoader("files/"+uploaded_file.name+".pdf")
            data = loader.load()
            st.write(f"Loaded {len(data)} pages from the PDF.")

            # Initialize text splitter
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=2000,
                chunk_overlap=300,
                length_function=len
            )
            all_splits = text_splitter.split_documents(data)
            st.write(f"Document processed into {len(all_splits)} chunks.")
            if all_splits:
                st.write(f"First chunk content: {all_splits[0].page_content[:200]}...")  # Print first 200 characters
            else:
                st.write("Warning: No chunks were created. This might indicate an issue with the document or the text splitter.")

            # Create and persist the vector store
            embedding_function = OllamaEmbeddings(model="mistral")
            vectorstore = Chroma.from_documents(
                documents=all_splits,
                embedding=embedding_function,
                persist_directory='data'
            )
            vectorstore.persist()
            st.session_state.vectorstore = vectorstore
            st.write(f"Vectorstore created and persisted with {vectorstore._collection.count()} documents.")

    # Get or create vectorstore
    vectorstore = get_or_create_vectorstore()
    st.write(f"Vectorstore contains {vectorstore._collection.count()} documents.")

    # Set up the retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    # Initialize the QA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=st.session_state.llm,
        chain_type='stuff',
        retriever=retriever,
        verbose=True,
        chain_type_kwargs={
            "verbose": True,
            "prompt": st.session_state.prompt,
            "memory": ConversationBufferMemory(
                memory_key="history",
                return_messages=True,
                input_key="question"),
        }
    )

    # Chat input
    if user_input := st.chat_input("You:", key="user_input"):
        user_message = {"role": "user", "message": user_input}
        st.session_state.chat_history.append(user_message)
        with st.chat_message("user"):
            st.markdown(user_input)
        with st.chat_message("assistant"):
            with st.spinner("Assistant is typing..."):
                # Debug: Print retrieved documents
                retrieved_docs = retriever.get_relevant_documents(user_input)
                st.write(f"Retrieved {len(retrieved_docs)} documents.")
                if retrieved_docs:
                    st.write(f"First retrieved document: {retrieved_docs[0].page_content[:200]}...")  # Print first 200 characters
                else:
                    st.write("No documents were retrieved. This might indicate an issue with the vectorstore or the retrieval process.")
                
                # Additional debug information
                st.write(f"Vectorstore size: {vectorstore._collection.count()} documents")
                st.write(f"Query: {user_input}")
                
                # Proceed with QA chain only if documents were retrieved
                if retrieved_docs:
                    response = qa_chain(user_input)
                    message_placeholder = st.empty()
                    full_response = ""
                    for chunk in response['result'].split():
                        full_response += chunk + " "
                        time.sleep(0.05)
                        # Add a blinking cursor to simulate typing
                        message_placeholder.markdown(full_response + "â–Œ")
                    message_placeholder.markdown(full_response)
                else:
                    full_response = "I'm sorry, but I couldn't find any relevant information to answer your question. This might be due to an issue with the document processing or retrieval system."
                    st.write(full_response)

            chatbot_message = {"role": "assistant", "message": full_response}
            st.session_state.chat_history.append(chatbot_message)

else:
    st.write("Please upload a PDF file.")